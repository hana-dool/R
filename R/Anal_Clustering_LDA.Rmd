---
title: "Untitled"
output: github_document
editor_options: 
  chunk_output_type: console
---
# LDA 란?
**LDA 란?**<br>

- LDA 란 Discriminant Analysis 의 한 방법으로서 Classification 의 한 방법이다.<br>
- 그중에서 우리가 해볼것은 Fisher Linear discriminant 방법으로서 서로 다른 그룹을 잘 구별할 수 있게 해주는 coefficient(linear 직선) 을 찾는 방법이다. 
- 분류할 때에 그룹이 잘 나누어져야 하므로 그룹내 분산은 작아야하고, 그룹간 분산은 커야한다. 듯 maximize 그룹간ss/그룹내ss 를 하는 방향으로 vector a 를 찾게되고, 그 vector a 는 그룹들을 제일 잘 구분하는 직선이 된다.
  - '수학적' 으로 maximize 하는 vector a를 찾을 수 있다!
- 그리고 그러한 Vector a 를 많이 찾으면 더 정확하게 그룹간 ss 들을 measure 할 수 있다.(그룹이 3 개인경우, 하나의 직선으로만 구분하기에는 벅차다. )
  - 최대 그룹의 수 J - 1 만큼의 Axis(vector a) 를 추가할 수 있다.(각 vector a 들은 maximize 그룹간ss/그룹내ss을 계산해 주는 eigenvector 들이다. ) 그에 따라 최대 j-1 개의 Axis를 형성가능하고 그에따라 분류 가능
- 이때 LDA 의 Decision boundary 는 직선이다. 
  - 왜냐하면 가정사항이 각 class_k 의 pdf ~ N(mu_k,sigma) 로 등분산,normal 이기 때문이다. 
  - 그래서 결정경계는 Normal 이 나온다.

**가정사항** <br>

- 각 class 들은 등분산 Normal 의 분포를 가진다.

**장단점** <br>

- 장점 
  - 가정이 위반되도 어느정도는 Robust함
  - 필요한 모수의 수가 적어서 Calculation 이 빠름(mu_1...mu_k, sigma, pi_1 ... pi_k)
    - hat(mu_k) : class k 인 관측치 x 의 평균
    - hat(sigma) : 각 클래스내의 편차 제곱을 모두 더한뒤 N-k 로 나눔
    - hat(pi_k) : 전체 관측치에서 class k 의 비율로 측정
- 단점
  - 모델이 등분산을 가정해서 너무 딱딱함 
  - 등분산이 아닌경우(class 크기가 다름) 안좋다.

# Analysis
- 분석을 진행할 데이터는 차에 대한 데이터이다.
- 자동차들에 대한 데이터로서 각 Col 들이 의미하는 바를 적어보면
  - Mileage : 연비
  - Repair : 수리상태 점수(정보가 없어 이것이라고 추측중)
  - Headroom : 자동차 실내의 시트 면부터 천장까지의 높이
  - Seatclearance : 앞 좌석과 뒷좌석 거리
  - Trunk : 트렁크 공간
  - Diameter : U 턴 하기위해 필요한 지름
  - Gear Ratio : 기어의 비율
  - Campany : 본사의 국정(1:미국,2:일본,3:유럽)

```{r}
set.seed(131)
library(readxl)
data_raw = read_excel('Data/CarData.xls',col_names = TRUE)
data_raw = data.frame(data_raw)
head(data_raw)
```

## NA imputation
- Na 가 들어가 있다. 이 떄에 무작정 KNN 이나, 평균 등으로 채우는 것보다는 데이터의 Structure 를 보고 난 이후 결정해야 한다.

**Repair1978 의 관계 살펴보기**
```{r}
library(reshape)
library(ggplot2)
data_plot <- na.omit(data_raw) # NA 지우기
data_plot <- data_plot[,unlist(lapply(data_plot,is.numeric))] # Numeric 만 추출 
df_melt = melt(data_plot,id=c('REPAIR1978')) # melt
head(df_melt)
ggplot(df_melt,aes(x=value,y=REPAIR1978))+
   facet_wrap(~variable, scales = "free") +
   geom_jitter()

library(corrplot)
df_cor= cor(data_plot) 
corrplot(df_cor,method = 'square') # 1978,1977 끼리 연관 있어보이는듯
corrplot(df_cor,method = 'number')
```
데이터가 REPAIR값 끼리 연관성이 있어보인다.

```{r}
data_raw 
``` 
또한 차 종류마다 repair 의 값이 비슷해보인다. <br>
그러므로 차 회사에 따라 Repair 1978 의 값과 1977 의 값의 추이를 보고 비슷하게 맞추면 될 것이다. <br>

```{r}
data_raw[32,'REPAIR1977'] = 4 # Ford_Fiesta
data_raw[44,'REPAIR1977'] = 2 # Merc._Monarch
data_raw[56,'REPAIR1977'] = 3 # Plym._Horizon
data_raw = data_raw[-c(3,10,53,57,63),] # NA 가 두개인 데이터는 삭제
row.names(data_raw) <- NULL
data = data_raw[,-c(1,14)]
```

## LDA
```{r}
library('MASS')
data_fisher=data_raw[,-1]
fisher=lda(COMPANY~.,prior=c(1,1,1)/3,data=data_fisher)
# Prior 는 아무것도 모른다고 가정하면 c(1,1,1)/3 이 된다.
# defualt 는 데이터의 비율에 따라 추정하게 된다.
fisher
```
## confusion matrix
- predict 한 값은 posterior 에서 확률값으로 나타난다.
- 각 클래스 1,2,3 에 대해 예측한 확률값을 볼 수 있다.
- 그에 따라서 클리스 1,2,3 의 예측은 class 에서 볼 수 있다.
```{r}
pred = predict(fisher,data_fisher)
pred$posterior
pred$class
pred_class = predict(fisher,data_fisher)$class
# confusion matrix
table(data_fisher$COMPANY,pred_class)
```
## Lda Axis histogram
LDA 는 Axis 를 최대 J(class 갯수)-1 개 쓸 수 있다고 하였다. <br>
현재 분류하려는 class 가 3개 이므로 2개의 Axis 를 쓸 수 있다. <br>
각각의 Axis 마다 데이터가 projection 되면 어떤 histogram 을 그리는지를 시각적으로 확인하고, 잘 분류가 될 수 있는지를 체크할 수 있다. 
```{r}
# histogram 그리기
val=0
lda.values <- predict(fisher)
lda.values$x
ldahist(data = lda.values$x[,1], g=data_fisher$COMPANY, nbins=20)
ldahist(data = lda.values$x[,2], g=data_fisher$COMPANY, nbins=20)
```
- lda.values$x 는 실제 projection 의 값을 나타낸다. 이 때에 coefficient 의 절대적인 크기는 의미가 없으므로( (0.4,0.4) = (1,1) ) coef 와 data를 곱해  계산한 값이 달라도 당황하지 말자.<br>
- lda Axis 에 투영한 값들의 비교를 보면 확실히 Axis 들이 잘 구분하고 있음을 볼 수 있다.

## Result visualization
```{R}
# LD 값에 따라 분류된 모양
score=pred$x
plot(score[,1],score[,2],col=pred$class,
     cex=1.5,xlab='LD1',ylab='LD2',cex.lab =1.5,
     pch = data_raw$COMPANY+14)
```
회사의 국적에 따라 모양을 다르게 주었다. <br>